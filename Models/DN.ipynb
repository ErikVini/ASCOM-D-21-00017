{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypw5-7M5JG0M"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:21:47.731638Z",
     "start_time": "2021-11-30T14:21:47.727083Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "jeUbUmOhA6II",
    "outputId": "391695eb-6719-4829-d959-eb3e73d1116a"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graphs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "# Others\n",
    "from   math import ceil, floor\n",
    "import random as rd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# SKLearn\n",
    "import sklearn\n",
    "from sklearn.model_selection     import train_test_split\n",
    "from sklearn.model_selection     import KFold\n",
    "from sklearn.preprocessing       import StandardScaler, LabelEncoder\n",
    "\n",
    "# Get current time to save files\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJalE5P5JG0c"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:04:13.556031Z",
     "start_time": "2021-11-30T14:04:11.605527Z"
    },
    "code_folding": [
     133
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "sT3lXzIwJG0g",
    "outputId": "be81ecda-65b5-43d3-ba2c-4fa75d99b6e6"
   },
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "seed = 1994\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading CSVs\n",
    "All_Data = pd.read_csv('AllData.csv')\n",
    "\n",
    "# Preprocessing Data\n",
    "Al0, Al1 = All_Data['r_auto'].values >= 16,   All_Data['r_auto'].values <= 21\n",
    "Al2      = All_Data['nDet_auto']     >= 8    # Number of features\n",
    "Al3      = All_Data['PhotoFlag']     == 0     # No problems in photometry\n",
    "Al4      = All_Data['PROB_GAL']      >= 0.5   # Only galaxies\n",
    "Al5      = All_Data['z_SDSS']        >  1e-4  # Filter out SDSS stars\n",
    "Al7      = All_Data['zErr']          <  0.4   # Low z error\n",
    "Al8      = All_Data['class_SDSS']    != 'QSO' # No QSOs\n",
    "All_Data = All_Data[Al0 & Al1 & Al2 & Al3 & Al4 & Al5 & Al7 & Al8]\n",
    "\n",
    "# Calculating ellipticity (flattening)\n",
    "All_Data['Ellipticity'] = 1 - All_Data['B']/All_Data['A']\n",
    "\n",
    "# Defining column names\n",
    "Extra_F   = ['FWHM_n', 'MUMAX', 'Ellipticity']\n",
    "Features  = [s for s in All_Data.columns.values if (('auto' in s) or ('_mag' in s)) and not (s.startswith('e') or s.startswith('n') or s.endswith('err'))]\n",
    "Errors    = [s for s in All_Data.columns.values if (('auto' in s) or ('_mag' in s)) and (s.startswith('e') or s.endswith('err'))]\n",
    "Target    = ['z_SDSS']\n",
    "Target_er = ['zErr']\n",
    "zBPZ      = ['zb']\n",
    "\n",
    "for error in Errors:\n",
    "    All_Data.loc[All_Data[error] > 1, error] = 1 # Set errors > 1 to = 1\n",
    "    \n",
    "# Fix missing features\n",
    "for feature in Features:\n",
    "    All_Data.loc[All_Data[feature] < 0,  feature] = 0\n",
    "    All_Data.loc[All_Data[feature] > 50, feature] = 0\n",
    "\n",
    "All_Data['u-r']   = All_Data['uJAVA_auto'] - All_Data['r_auto']\n",
    "All_Data['378-r'] = All_Data['F378_auto']  - All_Data['r_auto']\n",
    "All_Data['395-r'] = All_Data['F395_auto']  - All_Data['r_auto']\n",
    "All_Data['410-r'] = All_Data['F410_auto']  - All_Data['r_auto']\n",
    "All_Data['430-r'] = All_Data['F430_auto']  - All_Data['r_auto']\n",
    "All_Data['g-r']   = All_Data['g_auto']     - All_Data['r_auto']\n",
    "All_Data['515-r'] = All_Data['F515_auto']  - All_Data['r_auto']\n",
    "All_Data['r-660'] = All_Data['r_auto']     - All_Data['F660_auto']\n",
    "All_Data['r-i']   = All_Data['r_auto']     - All_Data['i_auto']\n",
    "All_Data['r-861'] = All_Data['r_auto']     - All_Data['F861_auto']\n",
    "All_Data['r-z']   = All_Data['r_auto']     - All_Data['z_auto']\n",
    "All_Data['r-W1']  = All_Data['r_auto']     - All_Data['w1_mag']\n",
    "All_Data['r-W2']  = All_Data['r_auto']     - All_Data['w2_mag']\n",
    "\n",
    "Colors    = [s for s in All_Data.columns.values if ('-' in s)]\n",
    "\n",
    "# Fix colors from missing features\n",
    "for color in Colors:\n",
    "    All_Data.loc[All_Data[color] <= -10,  color] = -10\n",
    "    All_Data.loc[All_Data[color] >= 10,   color] = 10\n",
    "\n",
    "TrainingFeatures = Features + Extra_F + Colors\n",
    "\n",
    "print('# Features:\\n# %s' %(TrainingFeatures))\n",
    "print('# Target:\\n# %s' %Target)\n",
    "print('# Errors:\\n# %s' %Errors)\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "# Splitting in training and test samples\n",
    "TrainingSample, TestingSample = sklearn.model_selection.train_test_split(All_Data, test_size=0.30, random_state=seed)\n",
    "\n",
    "##############################################################\n",
    "# Verifying the number of objects in each sample\n",
    "print('# Training Sample Objects = %s, Percentage of total = %.3g%%' %(len(TrainingSample), (100*len(TrainingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "print('# Testing Sample Objects  = %s, Percentage of total = %.3g%%' %(len(TestingSample), (100*len(TestingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "print('# Total Sample Objects    = %s' %(len(TestingSample)+len(TrainingSample)))\n",
    "print()\n",
    "# Verifying the existence of duplicates in the samples\n",
    "print('# Number of matching rows between two samples (should be zero):')\n",
    "print('# Train/Test = %s' %len(pd.merge(pd.DataFrame(TrainingSample), pd.DataFrame(TestingSample), how='inner')))\n",
    "print()\n",
    "# Statistics\n",
    "print('# Train max Z = %s' %np.max(TrainingSample.z_SDSS))\n",
    "print('# Test max Z  = %s' %np.max(TestingSample.z_SDSS))\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "# Scaling Data\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "Scaled_Train_X = Scaler.fit_transform(TrainingSample[TrainingFeatures])\n",
    "Scaled_Train_X = pd.DataFrame(Scaled_Train_X, columns=[TrainingFeatures])\n",
    "\n",
    "Scaled_Test_X = Scaler.transform(TestingSample[TrainingFeatures])\n",
    "Scaled_Test_X = pd.DataFrame(Scaled_Test_X, columns=[TrainingFeatures])\n",
    "\n",
    "for feature in Features:\n",
    "    Scaled_Train_X.loc[TrainingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "    Scaled_Test_X .loc[TestingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "##############################################################\n",
    "# Binning the redshift value\n",
    "# The code below will generate Num_Bins bins between z=0 and the MaximumZ (rounded up) and name each bins as a number\n",
    "Num_Bins = 200\n",
    "\n",
    "# Get maximum Z between the samples\n",
    "MaximumZ = max(round(np.max(TrainingSample[Target].values), 2), round(np.max(TestingSample[Target].values), 2))\n",
    "print('# MaximumZ  = %s\\n# Bin width = %s' %(MaximumZ, MaximumZ/Num_Bins))\n",
    "\n",
    "# Creates 'Num_Bins' of redshift between 0 and MaximumZ\n",
    "TrainingSample['Bin']    = pd.cut(TrainingSample.z_SDSS, bins=np.linspace(0, MaximumZ, Num_Bins), labels=np.arange(0, Num_Bins-1, 1))\n",
    "TestingSample['Bin']     = pd.cut(TestingSample.z_SDSS,  bins=np.linspace(0, MaximumZ, Num_Bins), labels=np.arange(0, Num_Bins-1, 1))\n",
    "\n",
    "# Encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(TrainingSample['Bin'])\n",
    "\n",
    "# Convert integers to dummy variables (i.e. one hot encoded)\n",
    "Target_Bins = tf.keras.utils.to_categorical(encoded_Y, num_classes=Num_Bins, dtype='float32')\n",
    "\n",
    "##############################################################\n",
    "# Plot Feature Histograms\n",
    "Plot_Hists = 0\n",
    "if Plot_Hists == 1:\n",
    "  fig, ax = plt.subplots(figsize=(15,15))\n",
    "  plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "  Features_to_plot = Features + Extra_F + Colors + zBPZ + Target\n",
    "  print()\n",
    "  plt_idx = 1\n",
    "  for feature in Features_to_plot:\n",
    "      plt.subplot(7, 5, plt_idx)\n",
    "\n",
    "      Feature_min = min(np.min(TrainingSample[feature]), np.min(TestingSample[feature]))\n",
    "      Feature_max = max(np.max(TrainingSample[feature]), np.max(TestingSample[feature]))\n",
    "\n",
    "      plt.hist(TrainingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "      plt.hist(TestingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "\n",
    "      plt.yscale('log')\n",
    "\n",
    "      plt.xlabel(feature)\n",
    "\n",
    "      plt.grid(lw=.5)\n",
    "      plt_idx = plt_idx+1  \n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V_qLghkdvcK"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:04:13.560147Z",
     "start_time": "2021-11-30T14:04:13.557335Z"
    }
   },
   "outputs": [],
   "source": [
    "Input              = tf.keras.layers.Input\n",
    "Dense              = tf.keras.layers.Dense\n",
    "BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "K                  = tf.keras.backend\n",
    "Model              = tf.keras.models.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:04:13.732176Z",
     "start_time": "2021-11-30T14:04:13.561735Z"
    },
    "code_folding": [
     26
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7CMayMKo9ig8",
    "outputId": "e1d7ee76-1ba0-4574-9315-e4595b3c8d65"
   },
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "seed = 0\n",
    "rd.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Scaling Data:\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "Scaled_Train_X = Scaler.fit_transform(TrainingSample[TrainingFeatures])\n",
    "Scaled_Train_X = pd.DataFrame(Scaled_Train_X, columns=[TrainingFeatures])\n",
    "\n",
    "Scaled_Test_X = Scaler.transform(TestingSample[TrainingFeatures])\n",
    "Scaled_Test_X = pd.DataFrame(Scaled_Test_X, columns=[TrainingFeatures])\n",
    "\n",
    "# Fix missing features after scaling\n",
    "for feature in Features:\n",
    "    Scaled_Train_X.loc[TrainingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "    Scaled_Test_X .loc[TestingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "\n",
    "kernel  = 'he_normal'\n",
    "Neurons = 5\n",
    "Epochs  = 200\n",
    "\n",
    "K.clear_session()\n",
    "def DN_Model_Def(NumFeat):\n",
    "    Input_Mags = Input(shape=(NumFeat,), name='Input_Dimensions')\n",
    "\n",
    "    input_0 = Dense(Neurons*7, kernel_initializer=kernel, activation='relu', use_bias=False)(Input_Mags) \n",
    "    batch_0  = BatchNormalization(scale=False)(input_0)\n",
    "\n",
    "    hidden_1 = Dense(Neurons*6, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_0) \n",
    "    batch_1  = BatchNormalization(scale=False)(hidden_1)\n",
    "\n",
    "    hidden_2 = Dense(Neurons*5, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_1) \n",
    "    batch_2  = BatchNormalization(scale=False)(hidden_2)\n",
    "\n",
    "    hidden_3 = Dense(Neurons*5, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_2) \n",
    "    batch_3  = BatchNormalization(scale=False)(hidden_3)\n",
    "\n",
    "    hidden_4 = Dense(Neurons*5, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_3) \n",
    "    batch_4  = BatchNormalization(scale=False)(hidden_4)\n",
    "\n",
    "    hidden_5 = Dense(Neurons*4, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_4) \n",
    "    batch_5  = BatchNormalization(scale=False)(hidden_5)\n",
    "\n",
    "    hidden_a = Dense(Neurons*4, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_5) \n",
    "    batch_a  = BatchNormalization(scale=False)(hidden_a)\n",
    "\n",
    "    hidden_6 = Dense(Neurons*4, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_a) \n",
    "    batch_6  = BatchNormalization(scale=False)(hidden_6)\n",
    "\n",
    "    hidden_7 = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_6) \n",
    "    batch_7  = BatchNormalization(scale=False)(hidden_7)\n",
    "\n",
    "    hidden_8 = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_7) \n",
    "    batch_8  = BatchNormalization(scale=False)(hidden_8)\n",
    "\n",
    "    hidden_9 = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_8) \n",
    "    batch_9  = BatchNormalization(scale=False)(hidden_9)\n",
    "\n",
    "    last_layer = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_9) \n",
    "\n",
    "    pdf_output = Dense(Num_Bins, activation='softmax', name='PDF')(last_layer)\n",
    "\n",
    "    # Define model linking inputs to outputs\n",
    "    model = Model(inputs=Input_Mags, outputs=pdf_output)\n",
    "\n",
    "    # Compile model\n",
    "    # Note that each output has a specific loss and metric (you can also define two optimizers (check))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Nadam(lr=0.01),\n",
    "                metrics='categorical_accuracy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:11:26.175139Z",
     "start_time": "2021-11-30T14:04:14.216207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "i = 0\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "\n",
    "# Current time and output folder\n",
    "TimeNow = datetime.now().strftime(\"%d-%m-%Y/%Hh%Mm/\")\n",
    "Output_Dir = 'Results/DN/'+ TimeNow\n",
    "print(\"# Output_Dir = '%s'\" %Output_Dir)\n",
    "if os.path.isdir(Output_Dir) == False:\n",
    "    os.makedirs(Output_Dir)\n",
    "\n",
    "# Save each fit separately\n",
    "DN_Model     = {}\n",
    "DN_Model_Fit = {}\n",
    "for train, validation in kfold.split(Scaled_Train_X, encoded_Y):\n",
    "    print('##################################################################')    \n",
    "    print('# Training Sample Objects   = %s,\\tPercentage of total = %.3g%%' %(len(train), (100*len(train)/(len(train)+len(validation)))))\n",
    "    print('# Validation Sample Objects = %s,\\tPercentage of total = %.3g%%' %(len(validation), (100*len(validation)/(len(train)+len(validation)))))\n",
    "    print('# Testing Sample Objects    = %s,\\tPercentage of total = %.3g%%' %(len(Scaled_Test_X), (100*len(Scaled_Test_X)/(len(Scaled_Test_X)))))\n",
    "    print('#')\n",
    "    print('# Number of matching rows between two samples (should be zero):')\n",
    "    print('# Train/Validation = %s' %len(pd.merge(pd.DataFrame(Scaled_Train_X.values[train]), pd.DataFrame(Scaled_Train_X.values[validation]), how='inner')))\n",
    "    print('# Test/Validation  = %s' %len(pd.merge(pd.DataFrame(Scaled_Test_X.values),         pd.DataFrame(Scaled_Train_X.values[validation]), how='inner')))\n",
    "    print('# Train/Test       = %s' %len(pd.merge(pd.DataFrame(Scaled_Train_X.values[train]), pd.DataFrame(Scaled_Test_X.values), how='inner')))\n",
    "    print()\n",
    "    \n",
    "  \n",
    "    DN_Model[i] = DN_Model_Def(len(TrainingFeatures))\n",
    "    DN_Model_Fit[i] = DN_Model[i].fit(Scaled_Train_X[TrainingFeatures].iloc[train], Target_Bins[train],\n",
    "                                      validation_data=(Scaled_Train_X[TrainingFeatures].iloc[validation], Target_Bins[validation]),\n",
    "                                      epochs=Epochs, batch_size=256, verbose=0, callbacks=[TqdmCallback()])\n",
    "\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:11:27.214711Z",
     "start_time": "2021-11-30T14:11:26.176894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot and save Loss\n",
    "Folds = DN_Model_Fit.keys()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "plt_idx = 1\n",
    "for fold in Folds:\n",
    "    plt.subplot(2, 2, plt_idx)\n",
    "    \n",
    "    plt.plot(DN_Model_Fit[fold].history['loss'], lw=2, alpha=1, label='Training')\n",
    "    plt.plot(DN_Model_Fit[fold].history['val_loss'], lw=2, alpha=1, label='Validation')\n",
    "    plt.ylim(bottom=2, top=4)\n",
    "    \n",
    "    plt.ylabel('Loss (CCE)')\n",
    "    plt.xlabel('Epochs')\n",
    "    if plt_idx == 1:\n",
    "        plt.legend()\n",
    "\n",
    "    plt_idx = plt_idx+1\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.savefig(Output_Dir+'Loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:16:24.943398Z",
     "start_time": "2021-11-30T14:16:11.481945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "Save_Model = True\n",
    "Folds = DN_Model_Fit.keys()\n",
    "\n",
    "if Save_Model == True:\n",
    "    for fold in Folds:\n",
    "        # Save model in TF format\n",
    "        DN_Model[fold].save(Output_Dir+'Fold_%s' %fold, overwrite=True)\n",
    "\n",
    "        # Save training history\n",
    "        pd.DataFrame(DN_Model_Fit[fold].history).to_csv(Output_Dir+'/Seed'+str(seed)+'_Fold%s.csv' %fold, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T14:29:09.165003Z",
     "start_time": "2021-11-30T14:28:59.649568Z"
    },
    "code_folding": [
     4,
     9,
     20,
     30
    ]
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "from scipy import integrate # To calculate the CDF of objects\n",
    "\n",
    "# To calculate PDFs\n",
    "def Calc_PDF(x, Weights, Means, STDs):\n",
    "    PDF = np.sum(Weights*(1/(STDs*np.sqrt(2*np.pi))) * np.exp((-1/2) * ((x[:,None]-Means)**2)/(STDs)**2), axis=1)\n",
    "    return PDF/np.trapz(PDF, x)\n",
    "\n",
    "# General function to find the nearest idx of an item in a list\n",
    "def find_nearest_idx(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "# Step function\n",
    "def step(x,y):\n",
    "    return 1 * (x > y)\n",
    "\n",
    "# Calculate HPDCI per object\n",
    "# https://stackoverflow.com/questions/33345780/empirical-cdf-in-python-similiar-to-matlabs-one\n",
    "def Check_Intervals(x):\n",
    "    List, last = [[]], None\n",
    "    for elem in x:\n",
    "        if last is None or abs(last - elem) <= 1:\n",
    "            List[-1].append(elem)\n",
    "        else:\n",
    "            List.append([elem])\n",
    "        last = elem\n",
    "    return List\n",
    "\n",
    "def Calculate_HPDCI(x, pdf_object, zspec):\n",
    "    HPDCI_Indexes = list(np.where(pdf_object >= pdf_object[find_nearest_idx(x, zspec)])[0])\n",
    "    HPDCI_Indexes = Check_Intervals(HPDCI_Indexes)\n",
    "\n",
    "    Object_HPDCI = 0\n",
    "    for k in range(len(HPDCI_Indexes)):\n",
    "        Object_HPDCI += np.trapz(pdf_object[HPDCI_Indexes[k]], x[HPDCI_Indexes[k]])\n",
    "\n",
    "    return Object_HPDCI\n",
    "\n",
    "Folds = DN_Model.keys()\n",
    "\n",
    "if len(Folds) > 1:\n",
    "    print('# Predicting for %s folds' %len(Folds))\n",
    "\n",
    "    x = np.linspace(0, MaximumZ, 200)\n",
    "\n",
    "    Fold_PDFs    = {}\n",
    "    Fold_PhotoZs = {}\n",
    "    Fold_CRPS    = {}\n",
    "    Fold_PITs    = {}\n",
    "    Fold_Odds    = {}\n",
    "\n",
    "    for fold in tqdm(Folds):\n",
    "        Fold_PDFs[fold] = DN_Model[fold].predict(Scaled_Test_X)\n",
    "\n",
    "        # Calculate ZPhots, PITs, CRPS, and Odds per fold\n",
    "        Fold_PhotoZs[fold] = []\n",
    "        Fold_Odds[fold]    = []\n",
    "        Fold_PITs[fold]    = []\n",
    "        Fold_CRPS[fold]    = []\n",
    "\n",
    "        for i in range(len(Scaled_Test_X)):\n",
    "            # Get the PDF for the object i\n",
    "            Obj_PDF = Fold_PDFs[fold][i]\n",
    "            # Find an approximate photo-z\n",
    "            Zphot   = np.average(x, weights=Obj_PDF)\n",
    "            # Using the new x-grid, build the 'detailed' PDF and find a better photo-z\n",
    "            Fold_PhotoZs[fold].append(Zphot)\n",
    "\n",
    "            # From the Obj_PDF, calculate the CDF\n",
    "            Obj_CDF = integrate.cumtrapz(Obj_PDF, x, initial=0)\n",
    "            # Calculate the Odds of object i (arXiv 9811189, eq. 17. Also calculated as the integral of the PDF between z_peak +/- 0.02)\n",
    "            Fold_Odds[fold].append( Obj_CDF[find_nearest_idx(x, Fold_PhotoZs[fold][i]+0.02)] - Obj_CDF[find_nearest_idx(x, Fold_PhotoZs[fold][i]-0.02)] )\n",
    "            # Calculate the PIT of object i (arXiv 1608.08016, eq. 2)\n",
    "            Fold_PITs[fold].append( Obj_CDF[find_nearest_idx(x, TestingSample['z_SDSS'].values[i])] )\n",
    "            # Calculate the CRPS of object i (arXiv 1608.08016, eq. 4)\n",
    "            Fold_CRPS[fold].append( np.trapz((Obj_CDF - step(x, TestingSample['z_SDSS'].values[i]))**2, x) )\n",
    "\n",
    "    Result_DF = pd.DataFrame()\n",
    "    Result_DF['r_auto']      = TestingSample['r_auto'].values\n",
    "    Result_DF['z']           = TestingSample['z_SDSS'].values\n",
    "    Result_DF['class_SDSS']  = TestingSample['class_SDSS'].values\n",
    "    Result_DF['zml']         = np.mean([Fold_PhotoZs[fold] for fold in Folds], axis=0)\n",
    "    Result_DF['Odds']        = np.mean([Fold_Odds[fold] for fold in Folds], axis=0)\n",
    "    Result_DF['PIT']         = np.mean([Fold_PITs[fold] for fold in Folds], axis=0)\n",
    "    Result_DF['CRPS']        = np.mean([Fold_CRPS[fold] for fold in Folds], axis=0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ypw5-7M5JG0M",
    "3a99V4OqgfpF",
    "RJh2aErHAtln",
    "coaiAAa1A11f",
    "HViq60iRJ84s",
    "vJalE5P5JG0c",
    "XOeLJEgAv3di",
    "vGxspYB4v6fi",
    "zt0zFpC1v8BG",
    "34ZgNuPyv9xt",
    "bFA0xIeoLxdV",
    "S7gCY-DLqPfY",
    "YwouqKZ-LPan",
    "ldhmk6Nc02_I",
    "4NG-e59HOO-N",
    "aHysv2QCtKVH"
   ],
   "name": "Keras Classification Modified Train Test 8F.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow2]",
   "language": "python",
   "name": "conda-env-Tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
