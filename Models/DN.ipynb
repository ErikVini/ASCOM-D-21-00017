{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypw5-7M5JG0M"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T20:59:23.958335Z",
     "start_time": "2021-08-17T20:59:23.953985Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "jeUbUmOhA6II",
    "outputId": "391695eb-6719-4829-d959-eb3e73d1116a"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graphs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "# Others\n",
    "from   math import ceil, floor\n",
    "import random as rd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.keras import TqdmCallback\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# SKLearn\n",
    "import sklearn\n",
    "from sklearn.model_selection     import train_test_split\n",
    "from sklearn.model_selection     import KFold\n",
    "from sklearn.preprocessing       import StandardScaler, LabelEncoder\n",
    "\n",
    "# Get current time to save files\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJalE5P5JG0c"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T20:33:12.345638Z",
     "start_time": "2021-08-17T20:33:09.384224Z"
    },
    "code_folding": [
     133
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "sT3lXzIwJG0g",
    "outputId": "be81ecda-65b5-43d3-ba2c-4fa75d99b6e6"
   },
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "seed = 1994\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading CSVs\n",
    "All_Data = pd.read_csv('SPLUSDR1+SDSS+WISE_S82.csv')\n",
    "\n",
    "# Preprocessing Data\n",
    "Al0, Al1 = All_Data['r_auto'].values >= 16,   All_Data['r_auto'].values <= 21\n",
    "Al2      = All_Data['nDet_auto']     >= 8    # Number of features\n",
    "Al3      = All_Data['PhotoFlag']     == 0     # No problems in photometry\n",
    "Al4      = All_Data['PROB_GAL']      >= 0.5   # Only galaxies\n",
    "Al5      = All_Data['z_SDSS']        >  1e-4  # Filter out SDSS stars\n",
    "Al7      = All_Data['zErr']          <  0.4   # Low z error\n",
    "Al8      = All_Data['class_SDSS']    != 'QSO' # No QSOs\n",
    "All_Data = All_Data[Al0 & Al1 & Al2 & Al3 & Al4 & Al5 & Al7 & Al8]\n",
    "\n",
    "# Calculating ellipticity (flattening)\n",
    "All_Data['Ellipticity'] = 1 - All_Data['B']/All_Data['A']\n",
    "\n",
    "# Defining column names\n",
    "Extra_F   = ['FWHM_n', 'MUMAX', 'Ellipticity']\n",
    "Features  = [s for s in All_Data.columns.values if (('auto' in s) or ('_mag' in s)) and not (s.startswith('e') or s.startswith('n') or s.endswith('err'))]\n",
    "Errors    = [s for s in All_Data.columns.values if (('auto' in s) or ('_mag' in s)) and (s.startswith('e') or s.endswith('err'))]\n",
    "Target    = ['z_SDSS']\n",
    "Target_er = ['zErr']\n",
    "zBPZ      = ['zb']\n",
    "\n",
    "for error in Errors:\n",
    "    All_Data.loc[All_Data[error] > 1, error] = 1 # Set errors > 1 to = 1\n",
    "    \n",
    "# Fix missing features\n",
    "for feature in Features:\n",
    "    All_Data.loc[All_Data[feature] < 0,  feature] = 0\n",
    "    All_Data.loc[All_Data[feature] > 50, feature] = 0\n",
    "\n",
    "All_Data['u-g']   = All_Data['uJAVA_auto'] - All_Data['g_auto']\n",
    "All_Data['378-g'] = All_Data['F378_auto']  - All_Data['g_auto']\n",
    "All_Data['395-g'] = All_Data['F395_auto']  - All_Data['g_auto']\n",
    "All_Data['410-g'] = All_Data['F410_auto']  - All_Data['g_auto']\n",
    "All_Data['430-g'] = All_Data['F430_auto']  - All_Data['g_auto']\n",
    "All_Data['g-515'] = All_Data['g_auto']     - All_Data['F515_auto']\n",
    "All_Data['g-r']   = All_Data['g_auto']     - All_Data['r_auto']\n",
    "All_Data['g-660'] = All_Data['g_auto']     - All_Data['F660_auto']\n",
    "All_Data['g-i']   = All_Data['g_auto']     - All_Data['i_auto']\n",
    "All_Data['g-861'] = All_Data['g_auto']     - All_Data['F861_auto']\n",
    "All_Data['g-z']   = All_Data['g_auto']     - All_Data['z_auto']\n",
    "All_Data['g-W1']  = All_Data['g_auto']     - All_Data['w1_mag']\n",
    "All_Data['g-W2']  = All_Data['g_auto']     - All_Data['w2_mag']\n",
    "\n",
    "Colors    = [s for s in All_Data.columns.values if ('-' in s)]\n",
    "\n",
    "# Fix colors from missing features\n",
    "for color in Colors:\n",
    "    All_Data.loc[All_Data[color] <= -10,  color] = -10\n",
    "    All_Data.loc[All_Data[color] >= 10,   color] = 10\n",
    "\n",
    "TrainingFeatures = Features + Extra_F + Colors\n",
    "\n",
    "print('# Features:\\n# %s' %(TrainingFeatures))\n",
    "print('# Target:\\n# %s' %Target)\n",
    "print('# Errors:\\n# %s' %Errors)\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "# Splitting in training and test samples\n",
    "TrainingSample, TestingSample = sklearn.model_selection.train_test_split(All_Data, test_size=0.30, random_state=seed)\n",
    "\n",
    "##############################################################\n",
    "# Verifying the number of objects in each sample\n",
    "print('# Training Sample Objects = %s, Percentage of total = %.3g%%' %(len(TrainingSample), (100*len(TrainingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "print('# Testing Sample Objects  = %s, Percentage of total = %.3g%%' %(len(TestingSample), (100*len(TestingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "print('# Total Sample Objects    = %s' %(len(TestingSample)+len(TrainingSample)))\n",
    "print()\n",
    "# Verifying the existence of duplicates in the samples\n",
    "print('# Number of matching rows between two samples (should be zero):')\n",
    "print('# Train/Test = %s' %len(pd.merge(pd.DataFrame(TrainingSample), pd.DataFrame(TestingSample), how='inner')))\n",
    "print()\n",
    "# Statistics\n",
    "print('# Train max Z = %s' %np.max(TrainingSample.z_SDSS))\n",
    "print('# Test max Z  = %s' %np.max(TestingSample.z_SDSS))\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "# Scaling Data\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "Scaled_Train_X = Scaler.fit_transform(TrainingSample[TrainingFeatures])\n",
    "Scaled_Train_X = pd.DataFrame(Scaled_Train_X, columns=[TrainingFeatures])\n",
    "\n",
    "Scaled_Test_X = Scaler.transform(TestingSample[TrainingFeatures])\n",
    "Scaled_Test_X = pd.DataFrame(Scaled_Test_X, columns=[TrainingFeatures])\n",
    "\n",
    "for feature in Features:\n",
    "    Scaled_Train_X.loc[TrainingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "    Scaled_Test_X .loc[TestingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "##############################################################\n",
    "# Binning the redshift value\n",
    "# The code below will generate Num_Bins bins between z=0 and the MaximumZ (rounded up) and name each bins as a number\n",
    "Num_Bins = 200\n",
    "\n",
    "# Get maximum Z between the samples\n",
    "MaximumZ = max(round(np.max(TrainingSample[Target].values), 2), round(np.max(TestingSample[Target].values), 2))\n",
    "print('# MaximumZ  = %s\\n# Bin width = %s' %(MaximumZ, MaximumZ/Num_Bins))\n",
    "\n",
    "# Creates 'Num_Bins' of redshift between 0 and MaximumZ\n",
    "TrainingSample['Bin']    = pd.cut(TrainingSample.z_SDSS, bins=np.linspace(0, MaximumZ, Num_Bins), labels=np.arange(0, Num_Bins-1, 1))\n",
    "TestingSample['Bin']     = pd.cut(TestingSample.z_SDSS,  bins=np.linspace(0, MaximumZ, Num_Bins), labels=np.arange(0, Num_Bins-1, 1))\n",
    "\n",
    "# Encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(TrainingSample['Bin'])\n",
    "\n",
    "# Convert integers to dummy variables (i.e. one hot encoded)\n",
    "Target_Bins = tf.keras.utils.to_categorical(encoded_Y, num_classes=Num_Bins, dtype='float32')\n",
    "\n",
    "##############################################################\n",
    "# Plot Feature Histograms\n",
    "Plot_Hists = 0\n",
    "if Plot_Hists == 1:\n",
    "  fig, ax = plt.subplots(figsize=(15,15))\n",
    "  plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "  Features_to_plot = Features + Extra_F + Colors + zBPZ + Target\n",
    "  print()\n",
    "  plt_idx = 1\n",
    "  for feature in Features_to_plot:\n",
    "      plt.subplot(7, 5, plt_idx)\n",
    "\n",
    "      Feature_min = min(np.min(TrainingSample[feature]), np.min(TestingSample[feature]))\n",
    "      Feature_max = max(np.max(TrainingSample[feature]), np.max(TestingSample[feature]))\n",
    "\n",
    "      plt.hist(TrainingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "      plt.hist(TestingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "\n",
    "      plt.yscale('log')\n",
    "\n",
    "      plt.xlabel(feature)\n",
    "\n",
    "      plt.grid(lw=.5)\n",
    "      plt_idx = plt_idx+1  \n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V_qLghkdvcK"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T20:33:15.618762Z",
     "start_time": "2021-08-17T20:33:15.615842Z"
    }
   },
   "outputs": [],
   "source": [
    "Input              = tf.keras.layers.Input\n",
    "Dense              = tf.keras.layers.Dense\n",
    "BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "K                  = tf.keras.backend\n",
    "Model              = tf.keras.models.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T21:00:04.732424Z",
     "start_time": "2021-08-17T21:00:04.458854Z"
    },
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7CMayMKo9ig8",
    "outputId": "e1d7ee76-1ba0-4574-9315-e4595b3c8d65"
   },
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "seed = 0\n",
    "rd.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Scaling Data:\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "Scaled_Train_X = Scaler.fit_transform(TrainingSample[TrainingFeatures])\n",
    "Scaled_Train_X = pd.DataFrame(Scaled_Train_X, columns=[TrainingFeatures])\n",
    "\n",
    "Scaled_Test_X = Scaler.transform(TestingSample[TrainingFeatures])\n",
    "Scaled_Test_X = pd.DataFrame(Scaled_Test_X, columns=[TrainingFeatures])\n",
    "\n",
    "# Fix missing features after scaling\n",
    "for feature in Features:\n",
    "    Scaled_Train_X.loc[TrainingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "    Scaled_Test_X .loc[TestingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "\n",
    "kernel  = 'he_normal'\n",
    "Neurons = 5\n",
    "Epochs  = 200\n",
    "\n",
    "K.clear_session()\n",
    "def DN_Model_Def(NumFeat):\n",
    "    Input_Mags = Input(shape=(NumFeat,), name='Input_Dimensions')\n",
    "\n",
    "    input_0 = Dense(Neurons*7, kernel_initializer=kernel, activation='relu', use_bias=False)(Input_Mags) \n",
    "    batch_0  = BatchNormalization(scale=False)(input_0)\n",
    "\n",
    "    hidden_1 = Dense(Neurons*6, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_0) \n",
    "    batch_1  = BatchNormalization(scale=False)(hidden_1)\n",
    "\n",
    "    hidden_2 = Dense(Neurons*5, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_1) \n",
    "    batch_2  = BatchNormalization(scale=False)(hidden_2)\n",
    "\n",
    "    hidden_3 = Dense(Neurons*5, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_2) \n",
    "    batch_3  = BatchNormalization(scale=False)(hidden_3)\n",
    "\n",
    "    hidden_4 = Dense(Neurons*5, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_3) \n",
    "    batch_4  = BatchNormalization(scale=False)(hidden_4)\n",
    "\n",
    "    hidden_5 = Dense(Neurons*4, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_4) \n",
    "    batch_5  = BatchNormalization(scale=False)(hidden_5)\n",
    "\n",
    "    hidden_a = Dense(Neurons*4, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_5) \n",
    "    batch_a  = BatchNormalization(scale=False)(hidden_a)\n",
    "\n",
    "    hidden_6 = Dense(Neurons*4, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_a) \n",
    "    batch_6  = BatchNormalization(scale=False)(hidden_6)\n",
    "\n",
    "    hidden_7 = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_6) \n",
    "    batch_7  = BatchNormalization(scale=False)(hidden_7)\n",
    "\n",
    "    hidden_8 = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_7) \n",
    "    batch_8  = BatchNormalization(scale=False)(hidden_8)\n",
    "\n",
    "    hidden_9 = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_8) \n",
    "    batch_9  = BatchNormalization(scale=False)(hidden_9)\n",
    "\n",
    "    last_layer = Dense(Neurons*3, kernel_initializer=kernel, activation='relu', use_bias=False)(batch_9) \n",
    "\n",
    "    pdf_output = Dense(Num_Bins, activation='softmax', name='PDF')(last_layer)\n",
    "\n",
    "    # Define model linking inputs to outputs\n",
    "    model = Model(inputs=Input_Mags, outputs=pdf_output)\n",
    "\n",
    "    # Compile model\n",
    "    # Note that each output has a specific loss and metric (you can also define two optimizers (check))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Nadam(lr=0.01),\n",
    "                metrics='categorical_accuracy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T21:08:43.339818Z",
     "start_time": "2021-08-17T21:00:57.271223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "i = 0\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "\n",
    "# Save each fit separately\n",
    "DN_Model     = {}\n",
    "DN_Model_Fit = {}\n",
    "for train, validation in kfold.split(Scaled_Train_X, encoded_Y):\n",
    "    print('##################################################################')    \n",
    "    print('# Training Sample Objects   = %s,\\tPercentage of total = %.3g%%' %(len(train), (100*len(train)/(len(train)+len(validation)))))\n",
    "    print('# Validation Sample Objects = %s,\\tPercentage of total = %.3g%%' %(len(validation), (100*len(validation)/(len(train)+len(validation)))))\n",
    "    print('# Testing Sample Objects    = %s,\\tPercentage of total = %.3g%%' %(len(Scaled_Test_X), (100*len(Scaled_Test_X)/(len(Scaled_Test_X)))))\n",
    "    print('#')\n",
    "    print('# Number of matching rows between two samples (should be zero):')\n",
    "    print('# Train/Validation = %s' %len(pd.merge(pd.DataFrame(Scaled_Train_X.values[train]), pd.DataFrame(Scaled_Train_X.values[validation]), how='inner')))\n",
    "    print('# Test/Validation  = %s' %len(pd.merge(pd.DataFrame(Scaled_Test_X.values),         pd.DataFrame(Scaled_Train_X.values[validation]), how='inner')))\n",
    "    print('# Train/Test       = %s' %len(pd.merge(pd.DataFrame(Scaled_Train_X.values[train]), pd.DataFrame(Scaled_Test_X.values), how='inner')))\n",
    "    print()\n",
    "    \n",
    "  \n",
    "    DN_Model[i] = DN_Model_Def(len(TrainingFeatures))\n",
    "    DN_Model_Fit[i] = DN_Model[i].fit(Scaled_Train_X[TrainingFeatures].iloc[train], Target_Bins[train],\n",
    "                                      validation_data=(Scaled_Train_X[TrainingFeatures].iloc[validation], Target_Bins[validation]),\n",
    "                                      epochs=Epochs, batch_size=256, verbose=0, callbacks=[TqdmCallback()])\n",
    "\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T21:09:23.041977Z",
     "start_time": "2021-08-17T21:09:22.510932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "Folds = DN_Model_Fit.keys()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "plt_idx = 1\n",
    "for fold in Folds:\n",
    "    plt.subplot(2, 2, plt_idx)\n",
    "    \n",
    "    plt.plot(DN_Model_Fit[fold].history['loss'], lw=2, alpha=1, label='Training')\n",
    "    plt.plot(DN_Model_Fit[fold].history['val_loss'], lw=2, alpha=1, label='Validation')\n",
    "    plt.ylim(bottom=2, top=4)\n",
    "    \n",
    "    plt.ylabel('Loss (CCE)')\n",
    "    plt.xlabel('Epochs')\n",
    "    if plt_idx == 1:\n",
    "        plt.legend()\n",
    "\n",
    "    plt_idx = plt_idx+1\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.savefig('Loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T21:10:02.180975Z",
     "start_time": "2021-08-17T21:09:47.234991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "Save_Model = True\n",
    "Folds = DN_Model_Fit.keys()\n",
    "\n",
    "# Time of trainingE\n",
    "TimeNow = datetime.now().strftime(\".%d-%m-%Y %Hh%Mm/\")\n",
    "Save_Dir = 'DN/' + str(Epochs) + TimeNow\n",
    "print(Save_Dir)\n",
    "\n",
    "if os.path.exists(Save_Dir) == False: os.makedirs(Save_Dir)        \n",
    "\n",
    "if Save_Model == True:\n",
    "    for fold in Folds:\n",
    "        # Save model in TF format\n",
    "        DN_Model[fold].save(Save_Dir+'Fold_%s' %fold, overwrite=True)\n",
    "\n",
    "        # Save training history\n",
    "        pd.DataFrame(DN_Model_Fit[fold].history).to_csv(Save_Dir+'/Seed'+str(seed)+'_Fold%s.csv' %fold, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ypw5-7M5JG0M",
    "3a99V4OqgfpF",
    "RJh2aErHAtln",
    "coaiAAa1A11f",
    "HViq60iRJ84s",
    "vJalE5P5JG0c",
    "XOeLJEgAv3di",
    "vGxspYB4v6fi",
    "zt0zFpC1v8BG",
    "34ZgNuPyv9xt",
    "bFA0xIeoLxdV",
    "S7gCY-DLqPfY",
    "YwouqKZ-LPan",
    "ldhmk6Nc02_I",
    "4NG-e59HOO-N",
    "aHysv2QCtKVH"
   ],
   "name": "Keras Classification Modified Train Test 8F.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
