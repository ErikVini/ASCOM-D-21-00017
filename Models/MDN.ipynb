{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:22:45.999820Z",
     "start_time": "2021-11-30T12:22:33.331562Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category = FutureWarning)\n",
    "\n",
    "# Data\n",
    "import numpy  as np\n",
    "import pandas as pd; pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt; plt.style.use('seaborn-darkgrid')\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Others\n",
    "import math; import random as rd; import sys; import os; import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "import json\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# SKLearn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn import impute\n",
    "\n",
    "# Fix seeds\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Get current time to save files\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:22:46.007247Z",
     "start_time": "2021-11-30T12:22:46.001922Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Loading TensorFlow variables and defining functions\n",
    "print(tf.__version__) # Must be >2 for Tensorflow Probability\n",
    "\n",
    "Input              = tf.keras.layers.Input\n",
    "Dense              = tf.keras.layers.Dense\n",
    "BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "Dropout            = tf.keras.layers.Dropout\n",
    "Model              = tf.keras.models.Model\n",
    "Nadam              = tf.keras.optimizers.Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading, pre-processing and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:22:46.042286Z",
     "start_time": "2021-11-30T12:22:46.010946Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MakeSamples(All_Data, Test_Size=0.30, Bins=None, Plot_Hists=False, Seed=None, FixAfter=False, DisplayDFs=False, Show_Scaled=False):\n",
    "    print('##################################################################')\n",
    "    print('# Generating new samples')\n",
    "    print('##################################################################')\n",
    "    if Seed == None:\n",
    "        Seed = np.random.randint(0, 2**30)\n",
    "        np.random.seed(Seed)\n",
    "    else:\n",
    "        np.random.seed(Seed)\n",
    "    print('# Random seed             = %s' %Seed)\n",
    "    \n",
    "    TrainingSample, TestingSample = sklearn.model_selection.train_test_split(All_Data, test_size=Test_Size, random_state=Seed)\n",
    "    \n",
    "    print('# First 5 train. samples  = %s' %TrainingSample.index.values[:5])\n",
    "    print('# First 5 testing samples = %s' %TestingSample.index.values[:5])\n",
    "  \n",
    "    ##############################################################\n",
    "    # Verifying the number of objects in each sample\n",
    "    print('# Training Sample Objects = %s, Percentage of total = %.3g%%' %(len(TrainingSample), (100*len(TrainingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "    print('# Testing Sample Objects  = %s, Percentage of total = %.3g%%' %(len(TestingSample), (100*len(TestingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "    print('# Total Sample Objects    = %s' %(len(TestingSample)+len(TrainingSample)))\n",
    "    # Verifying the existence of duplicates in the samples\n",
    "    print('# Number of matching rows between two samples (should be zero):')\n",
    "    print('# Train/Test = %s' %len(pd.merge(pd.DataFrame(TrainingSample), pd.DataFrame(TestingSample), how='inner')))\n",
    "    # Statistics\n",
    "    print('# Train max Z = %s' %np.max(TrainingSample.z_SDSS))\n",
    "    print('# Test max Z  = %s' %np.max(TestingSample.z_SDSS))\n",
    "  \n",
    "    ##############################################################\n",
    "    # Display DFs\n",
    "    if DisplayDFs == True:\n",
    "        display(Train_Data[:3])\n",
    "        display(Test_Data[:3])\n",
    "  \n",
    "    ##############################################################\n",
    "    # Scaling Data\n",
    "    Scaler = StandardScaler()\n",
    "  \n",
    "    Scaled_Train_X = Scaler.fit_transform(TrainingSample[TrainingFeatures])\n",
    "    Scaled_Train_X = pd.DataFrame(Scaled_Train_X, columns=[TrainingFeatures])\n",
    "  \n",
    "    Scaled_Test_X = Scaler.transform(TestingSample[TrainingFeatures])\n",
    "    Scaled_Test_X = pd.DataFrame(Scaled_Test_X, columns=[TrainingFeatures])\n",
    "  \n",
    "    # Fix missing features after scaling\n",
    "    if FixAfter == True:\n",
    "        for feature in Features:\n",
    "            Scaled_Train_X.loc[TrainingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "            Scaled_Test_X .loc[TestingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "  \n",
    "    ##############################################################\n",
    "    # Plot Feature Histograms\n",
    "    if Plot_Hists == True:\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "        plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "        if Show_Scaled == False:\n",
    "            Features_to_plot = Features + Extra_F + Colors + zBPZ + Target\n",
    "        if Show_Scaled == True:\n",
    "            Features_to_plot = Features + Extra_F + Colors\n",
    "        print()\n",
    "        plt_idx = 1\n",
    "        for feature in Features_to_plot:\n",
    "            plt.subplot(7, 5, plt_idx)\n",
    "    \n",
    "            if Show_Scaled == False:\n",
    "                Feature_min = min(np.min(TrainingSample[feature]), np.min(TestingSample[feature]))\n",
    "                Feature_max = max(np.max(TrainingSample[feature]), np.max(TestingSample[feature]))\n",
    "      \n",
    "                plt.hist(TrainingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "                plt.hist(TestingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "    \n",
    "            if Show_Scaled == True:\n",
    "                Feature_min = min(np.min(Scaled_Train_X[[feature]].values), np.min(Scaled_Train_X[[feature]].values))\n",
    "                Feature_max = max(np.max(Scaled_Train_X[[feature]].values), np.max(Scaled_Train_X[[feature]].values))\n",
    "      \n",
    "                plt.hist(Scaled_Train_X[[feature]].values, lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "                plt.hist(Scaled_Test_X[[feature]].values, lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "    \n",
    "            plt.yscale('log')\n",
    "    \n",
    "            plt.xlabel(feature)\n",
    "    \n",
    "            plt.grid(lw=.5)\n",
    "            plt_idx = plt_idx+1  \n",
    "    \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "      \n",
    "    return TrainingSample, Scaled_Train_X, TestingSample, Scaled_Test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:36:47.498060Z",
     "start_time": "2021-11-30T12:36:46.882428Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Loading and pre-processing input data\n",
    "# Fix seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading CSVs\n",
    "All_Data = pd.read_csv('AllData.csv')\n",
    "\n",
    "# Preprocessing Data\n",
    "C0       = All_Data['r_auto']        .between(16,21)\n",
    "C1       = All_Data['PhotoFlag']     == 0\n",
    "C2       = All_Data['PROB_GAL']      >= 0.5\n",
    "C3       = All_Data['z_SDSS']        >= 1e-4\n",
    "C4       = All_Data['zErr']          <= 0.4 \n",
    "C5       = All_Data['class_SDSS']    != 'QSO'\n",
    "C6       = All_Data['nDet_auto']     >= 8\n",
    "All_Data = All_Data[C0 & C1 & C2 & C3 & C4 & C5 & C6]\n",
    "\n",
    "# Calculating ellipticity (flattening)\n",
    "All_Data['Ellipticity'] = 1 - All_Data['B']/All_Data['A']\n",
    "\n",
    "# Defining column names\n",
    "Extra_F   = ['FWHM_n', 'MUMAX', 'Ellipticity']\n",
    "Features  = [s for s in All_Data.columns.values if ('auto' in s or 'mag' in s) and not (s.startswith('e') or s.startswith('n') or s.startswith('s') or s.endswith('err'))]\n",
    "Errors    = [s for s in All_Data.columns.values if (('auto' in s) or ('_mag' in s)) and (s.startswith('e') or s.endswith('err'))]\n",
    "Target    = ['z_SDSS']\n",
    "Target_er = ['zErr']\n",
    "zBPZ      = ['zb']\n",
    "\n",
    "# Replace missing magnitudes by zero\n",
    "for feature in Features:\n",
    "  All_Data.loc[All_Data[feature] < 0,   feature] = 0\n",
    "  All_Data.loc[All_Data[feature] > 50,  feature] = 0\n",
    "\n",
    "# Calculate colours\n",
    "Reference_Band  = 'r_auto'\n",
    "Reference_Idx   = Features.index(Reference_Band)\n",
    "FeaturesToLeft  = Features[:Reference_Idx]\n",
    "FeaturesToRight = Features[(Reference_Idx+1):]\n",
    "\n",
    "for feature in FeaturesToLeft: # of Reference_Band\n",
    "  All_Data[feature+'-'+Reference_Band] = All_Data[feature] - All_Data[Reference_Band] \n",
    "for feature in FeaturesToRight:\n",
    "  All_Data[Reference_Band+'-'+feature] = All_Data[Reference_Band] - All_Data[feature]\n",
    "\n",
    "Colours = [s for s in All_Data.columns.values if ('-' in s and 'auto' in s)]\n",
    "\n",
    "# Fix colors from missing features\n",
    "for colour in Colours:\n",
    "    All_Data.loc[All_Data[colour] <= -10,  colour] = -10\n",
    "    All_Data.loc[All_Data[colour] >= 10,   colour] = 10\n",
    "\n",
    "TrainingFeatures =  Features + Colours + Extra_F\n",
    "\n",
    "print('# Features:\\n# %s' %(TrainingFeatures))\n",
    "print('# Target:\\n# %s' %Target)\n",
    "print('# Errors:\\n# %s' %Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:37:00.409080Z",
     "start_time": "2021-11-30T12:36:55.838086Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainingSample, Training_Data_Features, TestingSample, Testing_Data_Features = MakeSamples(All_Data, Test_Size=0.30, Bins=200, Plot_Hists=False, Seed=seed, FixAfter=True, Show_Scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:37:03.115713Z",
     "start_time": "2021-11-30T12:37:03.080819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining features\n",
    "Training_Data_Features = Training_Data_Features[TrainingFeatures].values\n",
    "Testing_Data_Features  = Testing_Data_Features[TrainingFeatures].values\n",
    "\n",
    "# Defining target\n",
    "Training_Data_Target = TrainingSample['z_SDSS'].values\n",
    "Testing_Data_Target = TestingSample['z_SDSS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:37:05.337619Z",
     "start_time": "2021-11-30T12:37:05.322686Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TFP\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Negative Log Likelihood loss\n",
    "def neglogik(y, p_y):\n",
    "    return -p_y.log_prob(y)\n",
    "\n",
    "DenseVariational = tfp.layers.DenseVariational\n",
    "\n",
    "# Posterior definition\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(tfd.Normal(loc=t[..., :n], scale=1e-5 + tf.nn.softplus(t[..., n:])), reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# Prior definition\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(tfd.Normal(loc=t, scale=.1), reinterpreted_batch_ndims=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T12:37:05.793215Z",
     "start_time": "2021-11-30T12:37:05.747538Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Fix seeds. Seed is defined in the import cell\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "lrelu      = tf.keras.layers.LeakyReLU()\n",
    "Units      = 196\n",
    "Epochs     = 500\n",
    "Num_Layers = 3\n",
    "Batch_Size = 256\n",
    "\n",
    "# MDN Output config\n",
    "num_components = 20 # Number of Gaussians in the output mixture\n",
    "event_shape = [1]   # Dimension of the output (1 because we want the photometric redshift)\n",
    "params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape) # Number of units needed in the layer before the MixtudeSameFamily layer\n",
    "\n",
    "lr        = 0.001\n",
    "clipvalue = 0.5\n",
    "clipnorm  = 0.5\n",
    "\n",
    "# Dense Variational model\n",
    "tf.keras.backend.clear_session()\n",
    "def Dense_Variational(TrainSampleSize):\n",
    "    Input_Layer = Input(shape=(np.shape(Training_Data_Features)[1],))\n",
    "    \n",
    "    for i in range(Num_Layers):\n",
    "        if i == 0:\n",
    "            l = DenseVariational(Units, posterior_mean_field, prior_trainable, kl_weight=1/TrainSampleSize, activation=lrelu)(Input_Layer)\n",
    "        else:\n",
    "            l = DenseVariational(Units, posterior_mean_field, prior_trainable, kl_weight=1/TrainSampleSize, activation=lrelu)(l)\n",
    "        l = BatchNormalization()(l)\n",
    "        l = Dropout(0.1)(l)\n",
    "\n",
    "    l = Dense(params_size)(l)\n",
    "    \n",
    "    out = tfp.layers.MixtureNormal(num_components, event_shape)(l)\n",
    "    \n",
    "    model = Model(inputs=Input_Layer, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(lr=lr, clipvalue=clipvalue, clipnorm=clipnorm), loss=neglogik)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T13:17:37.954662Z",
     "start_time": "2021-11-30T12:39:39.157353Z"
    },
    "code_folding": [
     14
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Fix seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Current time and output folder\n",
    "TimeNow = datetime.now().strftime(\"%d-%m-%Y/%Hh%Mm/\")\n",
    "Output_Dir = 'Results/MDN/'+ TimeNow\n",
    "print(\"# Output_Dir = '%s'\" %Output_Dir)\n",
    "if os.path.isdir(Output_Dir) == False:\n",
    "    os.makedirs(Output_Dir)\n",
    "        \n",
    "# K-Fold?\n",
    "Scheme = 'KFold'\n",
    "\n",
    "if Scheme == 'KFold':\n",
    "    i = 0\n",
    "    print('# K-Fold seed: %s' %seed)\n",
    "    kfold = sklearn.model_selection.KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Save each fit separately\n",
    "    Prob_Model     = {}\n",
    "    Prob_Model_Fit = {}\n",
    "    for train, validation in kfold.split(Training_Data_Features, Training_Data_Target):\n",
    "        print('##################################################################')\n",
    "        print('# Starting fold number %s' %i)\n",
    "        print('# Training start time: %s' %TimeNow)\n",
    "        print('# Output directory is  %s' %Output_Dir)\n",
    "        print('##################################################################')\n",
    "        print('# Training Sample Objects   = %s (%.3g%%)' %(len(train), (100*len(train)/(len(train)+len(validation)+len(Testing_Data_Features)))))\n",
    "        print('# Validation Sample Objects = %s (%.3g%%)' %(len(validation), (100*len(validation)/(len(train)+len(validation)+len(Testing_Data_Features)))))\n",
    "        print('# Testing Sample Objects    = %s (%.3g%%)' %(len(Testing_Data_Features), (100*len(Testing_Data_Features)/(len(train)+len(validation)+len(Testing_Data_Features)))))\n",
    "        print('#')\n",
    "        print('# Number of matching rows between two samples (should be zero):')\n",
    "        print('# Train/Validation = %s' % len(pd.merge(pd.DataFrame(Training_Data_Features[train]), pd.DataFrame(Training_Data_Features[validation]), how='inner')))\n",
    "        print('# Test/Validation  = %s' % len(pd.merge(pd.DataFrame(Testing_Data_Features), pd.DataFrame(Training_Data_Features[validation]), how='inner')))\n",
    "        print('# Train/Test       = %s' % len(pd.merge(pd.DataFrame(Training_Data_Features[train]), pd.DataFrame(Testing_Data_Features), how='inner')))\n",
    "        print()\n",
    "\n",
    "        # Custom metrics\n",
    "        warnings.filterwarnings('ignore', category = Warning)\n",
    "\n",
    "        # Compiling new model for each fold\n",
    "        Prob_Model[i] = Dense_Variational(len(Training_Data_Features))\n",
    "\n",
    "        # Fitting the model\n",
    "        Prob_Model_Fit[i] = Prob_Model[i].fit(Training_Data_Features[train], Training_Data_Target[train],\n",
    "                                              validation_data=(Training_Data_Features[validation], Training_Data_Target[validation]), \n",
    "                                              epochs=Epochs, batch_size=Batch_Size, verbose=0, callbacks=[TqdmCallback(verbose=0)])\n",
    "\n",
    "        i = i+1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T13:27:58.420050Z",
     "start_time": "2021-11-30T13:27:56.354765Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot and save Loss\n",
    "Folds = Prob_Model_Fit.keys()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "plt_idx = 1\n",
    "for fold in Folds:\n",
    "    plt.subplot(2, 2, plt_idx)\n",
    "    \n",
    "    plt.plot(Prob_Model_Fit[fold].history['loss'], lw=2, alpha=1, label='Training')\n",
    "    plt.plot(Prob_Model_Fit[fold].history['val_loss'], lw=2, alpha=1, label='Validation')\n",
    "    plt.ylim(top=-1, bottom=-2)\n",
    "    \n",
    "    plt.ylabel('Loss (NGL)')\n",
    "    plt.xlabel('Epochs')\n",
    "    if plt_idx == 1:\n",
    "        plt.legend()\n",
    "\n",
    "    plt_idx = plt_idx+1\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.savefig(Output_Dir+'Loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T13:28:40.021070Z",
     "start_time": "2021-11-30T13:28:10.964519Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "Save_Model = True\n",
    "Folds = Prob_Model_Fit.keys()\n",
    "\n",
    "if Save_Model == True:\n",
    "    for fold in Folds:\n",
    "        # Save model in TF format\n",
    "        Prob_Model[fold].save(Output_Dir+'Fold_%s' %fold, overwrite=True)\n",
    "\n",
    "        # Save training history\n",
    "        pd.DataFrame(Prob_Model_Fit[fold].history).to_csv(Output_Dir+'/Seed'+str(seed)+'_Fold%s.csv' %fold, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T13:58:02.744377Z",
     "start_time": "2021-11-30T13:50:43.179820Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "from scipy import integrate # To calculate the CDF of objects\n",
    "\n",
    "# To calculate PDFs\n",
    "def Calc_PDF(x, Weights, Means, STDs):\n",
    "    PDF = np.sum(Weights*(1/(STDs*np.sqrt(2*np.pi))) * np.exp((-1/2) * ((x[:,None]-Means)**2)/(STDs)**2), axis=1)\n",
    "    return PDF/np.trapz(PDF, x)\n",
    "\n",
    "# General function to find the nearest idx of an item in a list\n",
    "def find_nearest_idx(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "# Step function\n",
    "def step(x,y):\n",
    "    return 1 * (x > y)\n",
    "\n",
    "# Calculate HPDCI per object\n",
    "# https://stackoverflow.com/questions/33345780/empirical-cdf-in-python-similiar-to-matlabs-one\n",
    "def Check_Intervals(x):\n",
    "    List, last = [[]], None\n",
    "    for elem in x:\n",
    "        if last is None or abs(last - elem) <= 1:\n",
    "            List[-1].append(elem)\n",
    "        else:\n",
    "            List.append([elem])\n",
    "        last = elem\n",
    "    return List\n",
    "\n",
    "def Calculate_HPDCI(x, pdf_object, zspec):\n",
    "    HPDCI_Indexes = list(np.where(pdf_object >= pdf_object[find_nearest_idx(x, zspec)])[0])\n",
    "    HPDCI_Indexes = Check_Intervals(HPDCI_Indexes)\n",
    "\n",
    "    Object_HPDCI = 0\n",
    "    for k in range(len(HPDCI_Indexes)):\n",
    "        Object_HPDCI += np.trapz(pdf_object[HPDCI_Indexes[k]], x[HPDCI_Indexes[k]])\n",
    "\n",
    "    return Object_HPDCI\n",
    "\n",
    "Folds = Prob_Model.keys()\n",
    "\n",
    "if len(Folds) > 1:\n",
    "    print('# Predicting for %s folds' %len(Folds))\n",
    "\n",
    "    x = np.arange(0, 1+0.001, 0.001)\n",
    "    MC_each_fold = 500\n",
    "\n",
    "    Fold_Weights = {}\n",
    "    Fold_Means   = {}\n",
    "    Fold_STDs    = {}\n",
    "    Fold_PDFs    = {}\n",
    "    Fold_PhotoZs = {}\n",
    "    Fold_CRPS    = {}\n",
    "    Fold_PITs    = {}\n",
    "    Fold_Odds    = {}\n",
    "\n",
    "    for fold in tqdm(Folds):\n",
    "        MC_Weights = []\n",
    "        MC_Means   = []\n",
    "        MC_STDs    = []\n",
    "\n",
    "        for i in tqdm(range(MC_each_fold)):\n",
    "            Pred = Prob_Model[fold](Testing_Data_Features)\n",
    "\n",
    "            Weight = Pred.submodules[1].probs_parameter().numpy()                                                 # Weights\n",
    "            Mean   = Pred.submodules[0].mean().numpy().reshape(len(Testing_Data_Features), np.shape(Weight)[1])   # Means\n",
    "            Std    = Pred.submodules[0].stddev().numpy().reshape(len(Testing_Data_Features), np.shape(Weight)[1]) # Stds\n",
    "\n",
    "            Sorted_Weight_Index = np.flip(np.argsort(Weight, axis=1), axis=1)\n",
    "\n",
    "            MC_Weights.append(Weight[np.arange(len(Weight))[:,None], Sorted_Weight_Index]) # Appending them to a list, so we can take the median below\n",
    "            MC_Means.append(Mean[np.arange(len(Mean))[:,None], Sorted_Weight_Index])       # https://stackoverflow.com/questions/20103779/index-2d-numpy-array-by-a-2d-array-of-indices-without-loops\n",
    "            MC_STDs.append(Std[np.arange(len(Std))[:,None], Sorted_Weight_Index])          #\n",
    "\n",
    "        Fold_Weights[fold] = np.median(MC_Weights, axis=0)\n",
    "        Fold_Means  [fold] = np.median(MC_Means, axis=0)\n",
    "        Fold_STDs   [fold] = np.median(MC_STDs, axis=0)\n",
    "\n",
    "        # Calculate ZPhots, PITs, CRPS, and Odds per fold\n",
    "        Fold_PhotoZs[fold] = []\n",
    "        Fold_Odds[fold]    = []\n",
    "        Fold_PITs[fold]    = []\n",
    "        Fold_CRPS[fold]    = []\n",
    "\n",
    "        for i in range(len(Testing_Data_Features)):\n",
    "            # Get the PDF for the object i\n",
    "            Obj_PDF      = Calc_PDF(x, Fold_Weights[fold][i], Fold_Means[fold][i], Fold_STDs[fold][i])\n",
    "            # Find an approximate photo-z\n",
    "            Approx_Zphot = x[np.argmax(Obj_PDF)]\n",
    "            # Using the approx photo-z, define a new x-grid and calculate the PDF using it (this makes a 'detailed' PDF peak)\n",
    "            x_detailed   = np.linspace(Approx_Zphot-0.025, Approx_Zphot+0.025, 101)\n",
    "            # Using the new x-grid, build the 'detailed' PDF and find a better photo-z\n",
    "            Fold_PhotoZs[fold].append(x_detailed[np.argmax(Calc_PDF(x_detailed, Fold_Weights[fold][i], Fold_Means[fold][i], Fold_STDs[fold][i]))])\n",
    "\n",
    "            # From the Obj_PDF, calculate the CDF\n",
    "            Obj_CDF = integrate.cumtrapz(Obj_PDF, x, initial=0)\n",
    "            # Calculate the Odds of object i (arXiv 9811189, eq. 17. Also calculated as the integral of the PDF between z_peak +/- 0.02)\n",
    "            Fold_Odds[fold].append( Obj_CDF[find_nearest_idx(x, Fold_PhotoZs[fold][i]+0.02)] - Obj_CDF[find_nearest_idx(x, Fold_PhotoZs[fold][i]-0.02)] )\n",
    "            # Calculate the PIT of object i (arXiv 1608.08016, eq. 2)\n",
    "            Fold_PITs[fold].append( Obj_CDF[find_nearest_idx(x, TestingSample['z_SDSS'].values[i])] )\n",
    "            # Calculate the CRPS of object i (arXiv 1608.08016, eq. 4)\n",
    "            Fold_CRPS[fold].append( np.trapz((Obj_CDF - step(x, TestingSample['z_SDSS'].values[i]))**2, x) )\n",
    "\n",
    "    Final_Weights = np.median([Fold_Weights[fold] for fold in Folds], axis=0)\n",
    "    Final_Means   = np.median([Fold_Means[fold] for fold in Folds], axis=0)\n",
    "    Final_STDs    = np.median([Fold_STDs[fold] for fold in Folds], axis=0)\n",
    "\n",
    "    # PDF\n",
    "    PDF = tfd.MixtureSameFamily(mixture_distribution    = tfd.Categorical(probs=Final_Weights),\n",
    "                                components_distribution = tfd.Normal(loc=Final_Means, scale=Final_STDs))\n",
    "    PDF_STDs = PDF.stddev().numpy()\n",
    "\n",
    "    Final_PDFs = []\n",
    "    for i in range(len(Testing_Data_Features)):\n",
    "        Final_PDFs.append(Calc_PDF(x, Final_Weights[i], Final_Means[i], Final_STDs[i]))\n",
    "\n",
    "    # Calculate ZPhots, PITs, CRPS, and Odds per fold\n",
    "    Fine_ZPhot = []\n",
    "    Odds       = []\n",
    "    PITs       = []\n",
    "    CRPS       = []\n",
    "\n",
    "    for i in range(len(Testing_Data_Features)):\n",
    "        # Get the PDF for the object i\n",
    "        Obj_PDF      = Calc_PDF(x, Final_Weights[i], Final_Means[i], Final_STDs[i])\n",
    "        # Find an approximate photo-z\n",
    "        Approx_Zphot = x[np.argmax(Obj_PDF)]\n",
    "        # Using the approx photo-z, define a new x-grid and calculate the PDF using it (this makes a 'detailed' PDF peak)\n",
    "        x_detailed   = np.linspace(Approx_Zphot-0.025, Approx_Zphot+0.025, 101)\n",
    "        # Using the new x-grid, build the 'detailed' PDF and find a better photo-z\n",
    "        Fine_ZPhot.append(x_detailed[np.argmax(Calc_PDF(x_detailed, Final_Weights[i], Final_Means[i], Final_STDs[i]))])\n",
    "\n",
    "        # From the Obj_PDF, calculate the CDF\n",
    "        Obj_CDF = integrate.cumtrapz(Obj_PDF, x, initial=0)\n",
    "        # Calculate the Odds of object i (arXiv 9811189, eq. 17. Also calculated as the integral of the PDF between z_peak +/- 0.02)\n",
    "        Odds.append( Obj_CDF[find_nearest_idx(x, Fine_ZPhot[i]+0.02)] - Obj_CDF[find_nearest_idx(x, Fine_ZPhot[i]-0.02)] )\n",
    "        # Calculate the PIT of object i (arXiv 1608.08016, eq. 2)\n",
    "        PITs.append( Obj_CDF[find_nearest_idx(x, TestingSample['z_SDSS'].values[i])] )\n",
    "        # Calculate the CRPS of object i (arXiv 1608.08016, eq. 4)\n",
    "        CRPS.append( np.trapz((Obj_CDF - step(x, TestingSample['z_SDSS'].values[i]))**2, x) )\n",
    "\n",
    "    Result_DF = pd.DataFrame()\n",
    "    Result_DF['r_auto']      = TestingSample['r_auto'].values\n",
    "    Result_DF['z']           = TestingSample['z_SDSS'].values\n",
    "    Result_DF['class_SDSS']  = TestingSample['class_SDSS'].values\n",
    "    Result_DF['zml']         = Fine_ZPhot\n",
    "    Result_DF['Odds']        = Odds\n",
    "    Result_DF['PIT']         = PITs\n",
    "    Result_DF['CRPS']        = CRPS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow2]",
   "language": "python",
   "name": "conda-env-Tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
