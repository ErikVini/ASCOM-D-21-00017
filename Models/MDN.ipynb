{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:33.420958Z",
     "start_time": "2021-08-17T18:18:14.242473Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category = FutureWarning)\n",
    "\n",
    "# Data\n",
    "import numpy  as np\n",
    "import pandas as pd; pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt; plt.style.use('seaborn-darkgrid')\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Others\n",
    "import math; import random as rd; import sys; import os; import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "import json\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# SKLearn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn import impute\n",
    "\n",
    "# Fix seeds\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Get current time to save files\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:37.195557Z",
     "start_time": "2021-08-17T18:18:33.423671Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Loading TensorFlow variables and defining functions\n",
    "print(tf.__version__) # Must be >2 for Tensorflow Probability\n",
    "\n",
    "Input              = tf.keras.layers.Input\n",
    "Dense              = tf.keras.layers.Dense\n",
    "BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "Dropout            = tf.keras.layers.Dropout\n",
    "Model              = tf.keras.models.Model\n",
    "Nadam              = tf.keras.optimizers.Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading, pre-processing and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:37.236873Z",
     "start_time": "2021-08-17T18:18:37.202077Z"
    },
    "code_folding": [
     2,
     54
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MakeSamples(All_Data, Test_Size=0.30, Bins=None, Plot_Hists=False, Seed=None, FixAfter=False, DisplayDFs=False, Show_Scaled=False):\n",
    "  print('##################################################################')\n",
    "  print('# Generating new samples')\n",
    "  print('##################################################################')\n",
    "  if Seed == None:\n",
    "    Seed = np.random.randint(0, 2**30)\n",
    "    np.random.seed(Seed)\n",
    "  else:\n",
    "    np.random.seed(Seed)\n",
    "  print('# Random seed             = %s' %Seed)\n",
    "  \n",
    "  TrainingSample, TestingSample = sklearn.model_selection.train_test_split(All_Data, test_size=Test_Size, random_state=Seed)\n",
    "  \n",
    "  print('# First 5 train. samples  = %s' %TrainingSample.index.values[:5])\n",
    "  print('# First 5 testing samples = %s' %TestingSample.index.values[:5])\n",
    "\n",
    "  ##############################################################\n",
    "  # Verifying the number of objects in each sample\n",
    "  print('# Training Sample Objects = %s, Percentage of total = %.3g%%' %(len(TrainingSample), (100*len(TrainingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "  print('# Testing Sample Objects  = %s, Percentage of total = %.3g%%' %(len(TestingSample), (100*len(TestingSample)/(len(TrainingSample)+len(TestingSample)))))\n",
    "  print('# Total Sample Objects    = %s' %(len(TestingSample)+len(TrainingSample)))\n",
    "  # Verifying the existence of duplicates in the samples\n",
    "  print('# Number of matching rows between two samples (should be zero):')\n",
    "  print('# Train/Test = %s' %len(pd.merge(pd.DataFrame(TrainingSample), pd.DataFrame(TestingSample), how='inner')))\n",
    "  # Statistics\n",
    "  print('# Train max Z = %s' %np.max(TrainingSample.z_SDSS))\n",
    "  print('# Test max Z  = %s' %np.max(TestingSample.z_SDSS))\n",
    "\n",
    "  ##############################################################\n",
    "  # Display DFs\n",
    "  if DisplayDFs == True:\n",
    "    display(Train_Data[:3])\n",
    "    display(Test_Data[:3])\n",
    "\n",
    "  ##############################################################\n",
    "  # Scaling Data\n",
    "  Scaler = StandardScaler()\n",
    "\n",
    "  Scaled_Train_X = Scaler.fit_transform(TrainingSample[TrainingFeatures])\n",
    "  Scaled_Train_X = pd.DataFrame(Scaled_Train_X, columns=[TrainingFeatures])\n",
    "\n",
    "  Scaled_Test_X = Scaler.transform(TestingSample[TrainingFeatures])\n",
    "  Scaled_Test_X = pd.DataFrame(Scaled_Test_X, columns=[TrainingFeatures])\n",
    "\n",
    "  # Fix missing features after scaling\n",
    "  if FixAfter == True:\n",
    "    for feature in Features:\n",
    "        Scaled_Train_X.loc[TrainingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "        Scaled_Test_X .loc[TestingSample.reset_index(drop=True)[feature] == 0, feature] = 0\n",
    "\n",
    "  ##############################################################\n",
    "  # Plot Feature Histograms\n",
    "  if Plot_Hists == True:\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    if Show_Scaled == False:\n",
    "      Features_to_plot = Features + Extra_F + Colors + zBPZ + Target\n",
    "    if Show_Scaled == True:\n",
    "      Features_to_plot = Features + Extra_F + Colors\n",
    "    print()\n",
    "    plt_idx = 1\n",
    "    for feature in Features_to_plot:\n",
    "        plt.subplot(7, 5, plt_idx)\n",
    "\n",
    "        if Show_Scaled == False:\n",
    "          Feature_min = min(np.min(TrainingSample[feature]), np.min(TestingSample[feature]))\n",
    "          Feature_max = max(np.max(TrainingSample[feature]), np.max(TestingSample[feature]))\n",
    "\n",
    "          plt.hist(TrainingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "          plt.hist(TestingSample[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "\n",
    "        if Show_Scaled == True:\n",
    "          Feature_min = min(np.min(Scaled_Train_X[[feature]].values), np.min(Scaled_Train_X[[feature]].values))\n",
    "          Feature_max = max(np.max(Scaled_Train_X[[feature]].values), np.max(Scaled_Train_X[[feature]].values))\n",
    "\n",
    "          plt.hist(Scaled_Train_X[[feature]].values, lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "          plt.hist(Scaled_Test_X[[feature]].values, lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step')\n",
    "\n",
    "        plt.yscale('log')\n",
    "\n",
    "        plt.xlabel(feature)\n",
    "\n",
    "        plt.grid(lw=.5)\n",
    "        plt_idx = plt_idx+1  \n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "  return TrainingSample, Scaled_Train_X, TestingSample, Scaled_Test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:39.443726Z",
     "start_time": "2021-08-17T18:18:37.241847Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Loading and pre-processing input data\n",
    "# Fix seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading CSVs\n",
    "Filename = 'SPLUSDR1+SDSS+WISE_S82.csv'\n",
    "All_Data = pd.read_csv(Filename)\n",
    "\n",
    "# Preprocessing Data\n",
    "C0       = All_Data['r_auto']        .between(16,21)\n",
    "C1       = All_Data['PhotoFlag']     == 0\n",
    "C2       = All_Data['PROB_GAL']      >= 0.5\n",
    "C3       = All_Data['z_SDSS']        >= 1e-4\n",
    "C4       = All_Data['zErr']          <= 0.4 \n",
    "C5       = All_Data['class_SDSS']    != 'QSO'\n",
    "C6       = All_Data['nDet_auto']     >= 8\n",
    "All_Data = All_Data[C0 & C1 & C2 & C3 & C4 & C5 & C6]\n",
    "\n",
    "# Calculating ellipticity (flattening)\n",
    "All_Data['Ellipticity'] = 1 - All_Data['B']/All_Data['A']\n",
    "\n",
    "# Defining column names\n",
    "Extra_F   = ['FWHM_n', 'MUMAX', 'Ellipticity']\n",
    "Features  = [s for s in All_Data.columns.values if ('auto' in s or 'mag' in s) and not (s.startswith('e') or s.startswith('n') or s.startswith('s') or s.endswith('err'))]\n",
    "Errors    = [s for s in All_Data.columns.values if (('auto' in s) or ('_mag' in s)) and (s.startswith('e') or s.endswith('err'))]\n",
    "Target    = ['z_SDSS']\n",
    "Target_er = ['zErr']\n",
    "zBPZ      = ['zb']\n",
    "\n",
    "# Replace missing magnitudes by zero\n",
    "for feature in Features:\n",
    "  All_Data.loc[All_Data[feature] < 0,   feature] = 0\n",
    "  All_Data.loc[All_Data[feature] > 50,  feature] = 0\n",
    "\n",
    "# Calculate colours\n",
    "Reference_Band  = 'r_auto'\n",
    "Reference_Idx   = Features.index(Reference_Band)\n",
    "FeaturesToLeft  = Features[:Reference_Idx]\n",
    "FeaturesToRight = Features[(Reference_Idx+1):]\n",
    "\n",
    "for feature in FeaturesToLeft: # of Reference_Band\n",
    "  All_Data[feature+'-'+Reference_Band] = All_Data[feature] - All_Data[Reference_Band] \n",
    "for feature in FeaturesToRight:\n",
    "  All_Data[Reference_Band+'-'+feature] = All_Data[Reference_Band] - All_Data[feature]\n",
    "\n",
    "Colours = [s for s in All_Data.columns.values if ('-' in s and 'auto' in s)]\n",
    "\n",
    "# Fix colors from missing features\n",
    "for colour in Colours:\n",
    "    All_Data.loc[All_Data[colour] <= -10,  colour] = -10\n",
    "    All_Data.loc[All_Data[colour] >= 10,   colour] = 10\n",
    "\n",
    "TrainingFeatures =  Features + Colours + Extra_F\n",
    "\n",
    "print('# Features:\\n# %s' %(TrainingFeatures))\n",
    "print('# Target:\\n# %s' %Target)\n",
    "print('# Errors:\\n# %s' %Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:45.532904Z",
     "start_time": "2021-08-17T18:18:39.449817Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainingSample, Training_Data_Features, TestingSample, Testing_Data_Features = MakeSamples(All_Data, Test_Size=0.30, Bins=200, Plot_Hists=False, Seed=seed, FixAfter=True, Show_Scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:45.564838Z",
     "start_time": "2021-08-17T18:18:45.537239Z"
    }
   },
   "outputs": [],
   "source": [
    "Training_Data_Features = Training_Data_Features[TrainingFeatures].values\n",
    "Testing_Data_Features  = Testing_Data_Features[TrainingFeatures].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:18:45.583724Z",
     "start_time": "2021-08-17T18:18:45.571719Z"
    }
   },
   "outputs": [],
   "source": [
    "Training_Data_Target = TrainingSample['z_SDSS'].values\n",
    "Testing_Data_Target = TestingSample['z_SDSS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:19:58.531811Z",
     "start_time": "2021-08-17T18:19:58.525172Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# TFP\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Negative Log Likelihood loss\n",
    "def neglogik(y, p_y):\n",
    "    return -p_y.log_prob(y)\n",
    "\n",
    "DenseVariational = tfp.layers.DenseVariational\n",
    "\n",
    "# Posterior definition\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(tfd.Normal(loc=t[..., :n], scale=1e-5 + tf.nn.softplus(t[..., n:])), reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# Prior definition\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(tfd.Normal(loc=t, scale=.1), reinterpreted_batch_ndims=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T18:20:00.158286Z",
     "start_time": "2021-08-17T18:20:00.140898Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Fix seeds. Seed is defined in the import cell\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "elu = tf.keras.layers.LeakyReLU()\n",
    "Units = 196\n",
    "Epochs = 500\n",
    "\n",
    "# MDN Output config\n",
    "num_components = 20 # Number of Gaussians in the output mixture\n",
    "event_shape = [1]   # Dimension of the output (1 because we want the photometric redshift)\n",
    "params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape) # Number of units needed in the layer before the MixtudeSameFamily layer\n",
    "\n",
    "lr        = 0.001\n",
    "clipvalue = 0.5\n",
    "clipnorm  = 0.5\n",
    "\n",
    "# Dense Variational model\n",
    "tf.keras.backend.clear_session()\n",
    "def Dense_Variational(TrainSampleSize):\n",
    "    Input_Layer = Input(shape=(np.shape(Training_Data_Features)[1],))\n",
    "    \n",
    "    l = DenseVariational(Units, posterior_mean_field, prior_trainable, kl_weight=1/TrainSampleSize, activation=lrelu)(Input_Layer)\n",
    "    l = BatchNormalization()(l)\n",
    "    l = Dropout(0.1)(l)\n",
    "\n",
    "    l = DenseVariational(Units, posterior_mean_field, prior_trainable, kl_weight=1/TrainSampleSize, activation=lrelu)(l)\n",
    "    l = BatchNormalization()(l)\n",
    "    l = Dropout(0.1)(l)\n",
    "\n",
    "    l = DenseVariational(Units, posterior_mean_field, prior_trainable, kl_weight=1/TrainSampleSize, activation=lrelu)(l)\n",
    "    l = BatchNormalization()(l)\n",
    "    l = Dropout(0.1)(l)\n",
    "\n",
    "    l = Dense(params_size)(l)\n",
    "    \n",
    "    out = tfp.layers.MixtureNormal(num_components, event_shape)(l)\n",
    "    \n",
    "    model = Model(inputs=Input_Layer, outputs=out)\n",
    "    model.compile(optimizer=optimizers.Nadam(lr=lr, clipvalue=clipvalue, clipnorm=clipnorm), loss=negloglik)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T19:54:50.350731Z",
     "start_time": "2021-08-17T18:20:04.432064Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Fix seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# K-Fold?\n",
    "Scheme = 'KFold'\n",
    "\n",
    "if Scheme == 'KFold':\n",
    "    i = 0\n",
    "    print('# K-Fold seed: %s' %seed)\n",
    "    kfold = sklearn.model_selection.KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Save each fit separately\n",
    "    Prob_Model     = {}\n",
    "    Prob_Model_Fit = {}\n",
    "    for train, validation in kfold.split(Training_Data_Features, Training_Data_Target):\n",
    "        print('##################################################################')\n",
    "        print('# Starting fold number %s' %i)\n",
    "        print('# Training start time: %s' %TimeNow)\n",
    "        print('# Output directory is  %s' %Output_Dir)\n",
    "        print('##################################################################')\n",
    "        print('# Training Sample Objects   = %s (%.3g%%)' %(len(train), (100*len(train)/(len(train)+len(validation)+len(Testing_Data_Features)))))\n",
    "        print('# Validation Sample Objects = %s (%.3g%%)' %(len(validation), (100*len(validation)/(len(train)+len(validation)+len(Testing_Data_Features)))))\n",
    "        print('# Testing Sample Objects    = %s (%.3g%%)' %(len(Testing_Data_Features), (100*len(Testing_Data_Features)/(len(train)+len(validation)+len(Testing_Data_Features)))))\n",
    "        print('#')\n",
    "        print('# Number of matching rows between two samples (should be zero):')\n",
    "        print('# Train/Validation = %s' % len(pd.merge(pd.DataFrame(Training_Data_Features[train]), pd.DataFrame(Training_Data_Features[validation]), how='inner')))\n",
    "        print('# Test/Validation  = %s' % len(pd.merge(pd.DataFrame(Testing_Data_Features), pd.DataFrame(Training_Data_Features[validation]), how='inner')))\n",
    "        print('# Train/Test       = %s' % len(pd.merge(pd.DataFrame(Training_Data_Features[train]), pd.DataFrame(Testing_Data_Features), how='inner')))\n",
    "        print()\n",
    "\n",
    "        # Custom metrics\n",
    "        warnings.filterwarnings('ignore', category = Warning)\n",
    "\n",
    "        # Compiling new model for each fold\n",
    "        Prob_Model[i] = Dense_Variational(len(Training_Data_Features))\n",
    "\n",
    "        # Fitting the model\n",
    "        Prob_Model_Fit[i] = Prob_Model[i].fit(Training_Data_Features[train], Training_Data_Target[train],\n",
    "                                              validation_data=(Training_Data_Features[validation], Training_Data_Target[validation]), \n",
    "                                              epochs=Epochs, batch_size=256, verbose=0, callbacks=[TqdmCallback(verbose=0)])\n",
    "\n",
    "        i = i+1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T19:56:00.591750Z",
     "start_time": "2021-08-17T19:55:59.175413Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot and save Loss\n",
    "Folds = Prob_Model_Fit.keys()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "plt_idx = 1\n",
    "for fold in Folds:\n",
    "    plt.subplot(2, 2, plt_idx)\n",
    "    \n",
    "    plt.plot(Prob_Model_Fit[fold].history['loss'], lw=2, alpha=1, label='Training')\n",
    "    plt.plot(Prob_Model_Fit[fold].history['val_loss'], lw=2, alpha=1, label='Validation')\n",
    "    plt.ylim(top=-1, bottom=-2)\n",
    "    \n",
    "    plt.ylabel('Loss (NGL)')\n",
    "    plt.xlabel('Epochs')\n",
    "    if plt_idx == 1:\n",
    "        plt.legend()\n",
    "\n",
    "    plt_idx = plt_idx+1\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.savefig('Loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T20:20:34.412636Z",
     "start_time": "2021-08-17T20:20:03.363315Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "Save_Model = True\n",
    "Folds = Prob_Model_Fit.keys()\n",
    "\n",
    "# Time of trainingE\n",
    "TimeNow = datetime.now().strftime(\"/%d-%m-%Y %Hh%Mm/\")\n",
    "Save_Dir = 'MDN/' + str(num_components) + 'Comp/' + str(Units) + '_' + str(Epochs) + TimeNow\n",
    "print(Save_Dir)\n",
    "\n",
    "if os.path.exists(Save_Dir) == False: os.makedirs(Save_Dir)        \n",
    "\n",
    "if Save_Model == True:\n",
    "    for fold in Folds:\n",
    "        # Save model in TF format\n",
    "        Prob_Model[fold].save(Save_Dir+'Fold_%s' %fold, overwrite=True)\n",
    "\n",
    "        # Save training history\n",
    "        pd.DataFrame(Prob_Model_Fit[fold].history).to_csv(Save_Dir+'/Seed'+str(seed)+'_Fold%s.csv' %fold, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
